{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "75ae7750",
   "metadata": {},
   "source": [
    "\n",
    "# Telco Customer Churn Prediction\n",
    "\n",
    "Churn prediction is a critical aspect for businesses, especially in the subscription-based sector, to retain their customers and enhance profitability. In this notebook, we will be analyzing a dataset of telco customers and developing a predictive model to understand the key factors that influence customer churn. \n",
    "\n",
    "## Objectives\n",
    "- Perform exploratory data analysis (EDA) to understand the data distribution and key characteristics.\n",
    "- Preprocess the data including feature selection, transformation, and scaling.\n",
    "- Train several machine learning models and select the one with the best performance.\n",
    "- Interpret the results to understand the most important features driving churn.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c3cb270",
   "metadata": {},
   "source": [
    "\n",
    "# Step 1: Importing Essential Libraries\n",
    "\n",
    "In this step, we import the essential libraries required for data manipulation, visualization, and model building.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "adecd04d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing Essential Libraries\n",
    "# Import the core libraries for data manipulation (pandas), numerical computation (numpy), \n",
    "# and visualization (matplotlib, seaborn) that we will need for EDA and model building.\n",
    "# Import others libraries for processing and moduling \n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import chi2_contingency\n",
    "from sklearn.cluster import KMeans\n",
    "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
    "import statsmodels.api as sm\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import  StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from xgboost import XGBClassifier\n",
    "from lightgbm import LGBMClassifier\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.metrics import roc_curve, roc_auc_score, accuracy_score, recall_score, precision_score, ConfusionMatrixDisplay, confusion_matrix, classification_report, f1_score\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "import joblib\n",
    "\n",
    "\n",
    "# Setting global plot style for consistency\n",
    "sns.set(style=\"whitegrid\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16763019",
   "metadata": {},
   "source": [
    "\n",
    "# Step 2: Data Loading and Initial Exploration\n",
    "\n",
    "Here, we load the Telco Customer Churn dataset and conduct initial exploration to understand the data types, number of columns, and any missing values.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1cddf3cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 7043 entries, 0 to 7042\n",
      "Data columns (total 33 columns):\n",
      " #   Column             Non-Null Count  Dtype  \n",
      "---  ------             --------------  -----  \n",
      " 0   CustomerID         7043 non-null   object \n",
      " 1   Count              7043 non-null   int64  \n",
      " 2   Country            7043 non-null   object \n",
      " 3   State              7043 non-null   object \n",
      " 4   City               7043 non-null   object \n",
      " 5   Zip Code           7043 non-null   int64  \n",
      " 6   Lat Long           7043 non-null   object \n",
      " 7   Latitude           7043 non-null   float64\n",
      " 8   Longitude          7043 non-null   float64\n",
      " 9   Gender             7043 non-null   object \n",
      " 10  Senior Citizen     7043 non-null   object \n",
      " 11  Partner            7043 non-null   object \n",
      " 12  Dependents         7043 non-null   object \n",
      " 13  Tenure Months      7043 non-null   int64  \n",
      " 14  Phone Service      7043 non-null   object \n",
      " 15  Multiple Lines     7043 non-null   object \n",
      " 16  Internet Service   7043 non-null   object \n",
      " 17  Online Security    7043 non-null   object \n",
      " 18  Online Backup      7043 non-null   object \n",
      " 19  Device Protection  7043 non-null   object \n",
      " 20  Tech Support       7043 non-null   object \n",
      " 21  Streaming TV       7043 non-null   object \n",
      " 22  Streaming Movies   7043 non-null   object \n",
      " 23  Contract           7043 non-null   object \n",
      " 24  Paperless Billing  7043 non-null   object \n",
      " 25  Payment Method     7043 non-null   object \n",
      " 26  Monthly Charges    7043 non-null   float64\n",
      " 27  Total Charges      7043 non-null   object \n",
      " 28  Churn Label        7043 non-null   object \n",
      " 29  Churn Value        7043 non-null   int64  \n",
      " 30  Churn Score        7043 non-null   int64  \n",
      " 31  CLTV               7043 non-null   int64  \n",
      " 32  Churn Reason       1869 non-null   object \n",
      "dtypes: float64(3), int64(6), object(24)\n",
      "memory usage: 1.8+ MB\n",
      "Missing values per column:\n",
      "CustomerID              0\n",
      "Count                   0\n",
      "Country                 0\n",
      "State                   0\n",
      "City                    0\n",
      "Zip Code                0\n",
      "Lat Long                0\n",
      "Latitude                0\n",
      "Longitude               0\n",
      "Gender                  0\n",
      "Senior Citizen          0\n",
      "Partner                 0\n",
      "Dependents              0\n",
      "Tenure Months           0\n",
      "Phone Service           0\n",
      "Multiple Lines          0\n",
      "Internet Service        0\n",
      "Online Security         0\n",
      "Online Backup           0\n",
      "Device Protection       0\n",
      "Tech Support            0\n",
      "Streaming TV            0\n",
      "Streaming Movies        0\n",
      "Contract                0\n",
      "Paperless Billing       0\n",
      "Payment Method          0\n",
      "Monthly Charges         0\n",
      "Total Charges           0\n",
      "Churn Label             0\n",
      "Churn Value             0\n",
      "Churn Score             0\n",
      "CLTV                    0\n",
      "Churn Reason         5174\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Data Loading\n",
    "# Loading the dataset into a pandas DataFrame to start our data analysis and model building.\n",
    "data = pd.read_excel(\"Telco_customer_churn.xlsx\")\n",
    "\n",
    "# Basic Data Exploration\n",
    "# Let's explore the structure of our dataset to understand the types of features available.\n",
    "data.info()  # Displays information about columns and data types\n",
    "\n",
    "# Checking for missing values\n",
    "print(\"Missing values per column:\")\n",
    "print(data.isna().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f7570a1",
   "metadata": {},
   "source": [
    "Here are some quick comments for the Data Loading and Initial Exploration step:\n",
    "\n",
    "1. **Dataset Size**: The dataset contains 7,043 entries across 33 columns. This is a moderately sized dataset suitable for typical machine learning models.\n",
    "\n",
    "2. **Data Types**: The dataset has mixed data types, including numerical (`int64`, `float64`) and categorical (`object`). Some columns like `Total Charges` are of type `object` and might need conversion for numerical analysis.\n",
    "\n",
    "3. **Non-Null Counts**: Most columns have no missing values, except for `Churn Reason`, which has a large number of missing entries (5,174 out of 7,043). We need to decide how to handle this columnâ€”either by imputing, dropping, or analyzing its impact.\n",
    "\n",
    "4. **Unique Identifiers**: Columns like `CustomerID` are unique identifiers and should be dropped for modeling purposes as they do not contribute to predicting churn.\n",
    "\n",
    "5. **Redundant Columns**: Some columns such as `Count`, `Lat Long`, `Zip Code`, and `Churn Score` may not provide additional predictive power and could be dropped during data cleaning.\n",
    "\n",
    "These observations will help guide the feature selection and preprocessing steps."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9d995a1",
   "metadata": {},
   "source": [
    "# Step 3: Data Cleaning and Preprocessing\n",
    "In this section, we will clean the dataset by handling missing values, dropping unnecessary features, and encoding categorical variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6c3a4fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify Categorical Columns\n",
    "numerical_cols = data.select_dtypes(include=['int64', 'float64']).columns\n",
    "print(f\"Categorical Columns: {numerical_cols}\")\n",
    "\n",
    "print(\"\\n-------numerical Values---------\")\n",
    "for col in numerical_cols:\n",
    "    print(f\"{col}: {data[col].unique()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b94480b6",
   "metadata": {},
   "source": [
    "I will drop ((Count)) variable, because it is has unique value and it does not contain any pattern or relationship with the target variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6aacee00",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.drop('Count',axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e6d33d6",
   "metadata": {},
   "source": [
    "I will CLTV (drop Customer Lifetime Value) variable because it is already a prediction based on many variables known to affect churn, including it in your machine learning model introduces target leakage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e24c0a12",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.drop('CLTV',axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc6902e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the correlation matrix for numerical features\n",
    "correlation_matrix = data.corr()\n",
    "\n",
    "# Plot a heatmap of the correlation matrix\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', fmt='.2f', linewidths=0.5)\n",
    "plt.title('Correlation Matrix')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e5bb3fd",
   "metadata": {},
   "source": [
    "#### Key Observations from the Correlation Matrix:\n",
    "- The Tenure Months is correlated negatively with the target about - 0.35, that indicates the cunstumers  with longer tenure are less likely to churn.\n",
    "- Monthly Charge is weak corrected positively with the target about 0.19, that indictes the cunstumers with Higher monthly charges slightly increase the likelihood of churn.\n",
    "- CLTV (Customer Lifetime Value) is  weak corrected negatively with the target about -0.13,that indictes the cunstumers with higher CLTV customers are slightly less likely to churn.\n",
    "- Churn Score is strong corrected positively with the target about 0.66, however, this is a data leakage risk if Churn Score is derived from or influenced by churn data.\n",
    "- Zip Code, Latitude, and Longitude:\n",
    " -- Zip Code & Latitude: 0.90\n",
    " -- Latitude & Longitude: -0.88\n",
    " --These high correlations indicate these geographical features are redundant.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4315556d",
   "metadata": {},
   "source": [
    "I will drop ((Churn Score)) variable because it is already a prediction based on many variables known to affect churn, including it in your machine learning model introduces target leakage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef6efd07",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.drop('Churn Score', axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fddd0e29",
   "metadata": {},
   "source": [
    "I will drop ip Code, Latitude, and Longitud because they are a redundant atures and are not directly predictive of churn (correlation with Churn Value is close to 0)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b061345a",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_city = data[['Latitude', 'Longitude']] #I will save this dataset to use it for creation the Feature Engineering for City.\n",
    "data.drop(['Zip Code', 'Latitude', 'Longitude'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa6d6e61",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify columns with only one unique value\n",
    "single_value_cols = [col for col in data.columns if data[col].nunique() == 1]\n",
    "\n",
    "# Print the columns with a single unique value\n",
    "print(f\"Columns with a single unique value: {single_value_cols}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29de7f14",
   "metadata": {},
   "source": [
    "I have to drop these variables: ((Count, Country, State)) because they have only one unique value across all rows.\n",
    "This columns don't add any useful information to the model because they have no variability. they are essentially constant \n",
    "for all data points and will not help differentiate between target outcomes.\n",
    "\n",
    "I will drop ((CustomerID)), because it is an Unique Identifiers and it does not contain any pattern or relationship with the target variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21adf063",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop these columns from the dataset\n",
    "data.drop(single_value_cols, axis=1, inplace=True)\n",
    "data.drop('CustomerID', axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "259a5597",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify columns with a binary types (type of categorical) and tronsform them to numerical 0 and 1\n",
    "binary_values_cols_catg = [col for col in data.columns if data[col].nunique() == 2 and data[col].dtype == 'object']\n",
    "# Print the columns with a binary values\n",
    "print(f\"-- Columns with a binary values: {binary_values_cols_catg}\")\n",
    "\n",
    "print(\"\\n-------Binary Values---------\")\n",
    "for col in binary_values_cols_catg:\n",
    "    print(f\"{col}: {data[col].unique()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e75b8917",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('these are the columns that we sill have :', data.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c1e9c64",
   "metadata": {},
   "source": [
    "\n",
    "# Step 4: Feature Engineering\n",
    "\n",
    "We perform feature engineering by encoding categorical variables. Binary features are transformed into numeric values, and one-hot encoding is used for multi-category features.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b89b9c9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encoding categorical variables into numerical representations is essential for most machine learning algorithms.\n",
    "# Binary features are mapped to 0 and 1 for simplicity.\n",
    "\n",
    "binary_cols = ['Senior Citizen', 'Partner', 'Dependents', 'Phone Service', 'Paperless Billing']\n",
    "for col in binary_cols:\n",
    "    data[col] = data[col].map({'Yes': 1, 'No': 0})\n",
    "\n",
    "# Encoding 'Gender' as a binary variable: Male -> 1, Female -> 0.\n",
    "data['Gender'] = data['Gender'].map({'Male': 1, 'Female': 0})\n",
    "\n",
    "# One-Hot Encoding for multi-category features such as Internet Service, Contract, and Payment Method.\n",
    "# One-hot encoding helps the model to interpret categorical variables effectively by creating binary columns for each category.\n",
    "data = pd.get_dummies(data, columns=['Internet Service', 'Contract', 'Payment Method'], drop_first=True)\n",
    "# Feature Engineering - Categorical Encoding\n",
    "# We convert categorical variables into numerical forms. Binary features are mapped to 0 and 1.\n",
    "binary_cols = ['Senior Citizen', 'Partner', 'Dependents', 'Phone Service', 'Paperless Billing']\n",
    "for col in binary_cols:\n",
    "    data[col] = data[col].map({'Yes': 1, 'No': 0})\n",
    "\n",
    "# Encoding 'Gender' into a binary feature: 1 for Male, 0 for Female\n",
    "data['Gender'] = data['Gender'].map({'Male': 1, 'Female': 0})\n",
    "\n",
    "# One-Hot Encoding for multi-category features such as Internet Service, Contract, and Payment Method\n",
    "data = pd.get_dummies(data, columns=['Internet Service', 'Contract', 'Payment Method'], drop_first=True)\n",
    "#I will drop Churn Label because it is the same of Churn value and it is our target\n",
    "data.drop('Churn Label', axis=1,inplace=True)\n",
    "# Transform them to 1/0 values\n",
    "for col in ['Senior Citizen', 'Partner', 'Dependents', 'Phone Service', 'Paperless Billing']:\n",
    "    data[col] = data[col].map({'Yes': 1, 'No': 0})\n",
    "\n",
    "# Binary encoding for Gender\n",
    "data['Gender'] = data['Gender'].map({'Male': 1, 'Female': 0})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81287047",
   "metadata": {},
   "outputs": [],
   "source": [
    "categorical_cols = data.select_dtypes(include=['object', 'category']).columns\n",
    "print(f\"Categorical Columns: {categorical_cols}\")\n",
    "\n",
    "print(\"\\n-------categorical Values---------\")\n",
    "for col in categorical_cols:\n",
    "    print(f\"{col}: {data[col].nunique()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc8d5c05",
   "metadata": {},
   "source": [
    "# Step 5: Feature Exploration and Cleaning base for further processing\n",
    "In this section, we will visualize some of the key features to understand their relationship with the target variable (Churn Value)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7e21d2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot each categorical feature against Churn Value\n",
    " for col in categorical_cols:\n",
    "        plt.figure(figsize=(8, 4))\n",
    "        sns.countplot(x=col, hue='Churn Value', data=data)\n",
    "        plt.title(f'Churn Distribution by {col}')\n",
    "        plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73ccfe07",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to plot stacked bar charts\n",
    "def plot_stacked_bar(column):\n",
    "    # Create a contingency table\n",
    "    crosstab = pd.crosstab(data[column], data['Churn Value'], normalize='index')\n",
    "\n",
    "    # Plot the stacked bar chart\n",
    "    crosstab.plot(kind='bar', stacked=True, figsize=(8, 4), colormap='coolwarm')\n",
    "    plt.title(f'Stacked Bar Plot: {column} vs Churn Value')\n",
    "    plt.xlabel(column)\n",
    "    plt.ylabel('Proportion')\n",
    "    plt.legend(title='Churn Value', loc='upper right')\n",
    "    plt.show()\n",
    "\n",
    "# Plot stacked bar charts for all categorical columns\n",
    "for col in categorical_cols:\n",
    "    plot_stacked_bar(col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b122450a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform Chi-Square test for each categorical feature\n",
    "for col in categorical_cols:\n",
    "    crosstab = pd.crosstab(data[col], data['Churn Value'])\n",
    "    chi2, p, dof, expected = chi2_contingency(crosstab)\n",
    "    \n",
    "    print(f'{col}: Chi2 = {chi2:.2f}, p = {p:.4f}')\n",
    "    if p < 0.05:\n",
    "        print(f\"  -> Significant relationship with Churn (p < 0.05)\\n\")\n",
    "    else:\n",
    "        print(f\"  -> No significant relationship with Churn (p >= 0.05)\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72da5912",
   "metadata": {},
   "source": [
    "- Multiple Lines (keep it): Customers with multiple lines or no phone service churn at similar rates.Single line customers seem to churn less frequently. Significant relationship with Churn (p < 0.05)\n",
    "- Internet Service (keep it): Fiber optic customers churn more than DSL customers. Customers with no internet service have minimal churn. Significant relationship with Churn (p < 0.05)\n",
    "- Online Security & oline Backup  & Device Protection & Tech Support (testing for multicollinearity (correlation between these features) during feature selection because they could be redundant): CMore churn for customers without these services. Again, no internet service customers rarely churn. Significant relationship with Churn (p < 0.05)\n",
    "- Streaming Movies & Streaming TV (testing for multicollinearity (correlation between these features) during feature selection because they could be redundant) : Observation: Both \"Yes\" and \"No\" categories have a similar churn proportion. Customers with no internet service have minimal churn. Significant relationship with Churn (p < 0.05)\n",
    "- Lat Long (drop it): The Lat Long is highly fragmented with too many unique values, making it hard to derive meaningful patterns from these visualizations. No significant relationship with Churn (p >= 0.05)\n",
    "- City (keep it): It has a significant relationship with Churn (p < 0.05)\n",
    "- Total Charges (drop it): The Total Charges values are scattered across a wide range, with no clear pattern in the churn distribution. No significant relationship with Churn (p >= 0.05)\n",
    "- Payment Method (keep it): Customers paying with Electronic Check show higher churn than those using Bank Transfer, Credit Card, or Mailed Check. Significant relationship with Churn (p < 0.05)\n",
    "- Contract (keep it): Month-to-Month contracts have higher churn compared to One-Year and Two-Year contracts. One-Year and Two-Year contracts have very low churn rates. Significant relationship with Churn (p < 0.05)\n",
    "- Churn Reason (drop it): It has a significant relationship with Churn (p < 0.05)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85d8fc64",
   "metadata": {},
   "outputs": [],
   "source": [
    "##Churn Reason, Lat Long\n",
    "data.drop(['Churn Reason', 'Lat Long'], axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67e48aa6",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.select_dtypes(include=['object', 'category']).columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1831eba8",
   "metadata": {},
   "outputs": [],
   "source": [
    "##Multiple Lines\n",
    "# Map Multiple Lines to 3 values\n",
    "data['Multiple Lines'] = data['Multiple Lines'].map({'Yes':1, 'No':0, 'No phone service':0})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a72ccad3",
   "metadata": {},
   "outputs": [],
   "source": [
    "##Internet Service, Contract,Payment Method\n",
    "# Apply one-hot encoding to these categorical features\n",
    "catg_to_oneHot = ['Internet Service', 'Contract','Payment Method']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf073fce",
   "metadata": {},
   "source": [
    "- City: I will Cluster Cities Using K-Means Based on Latitude and Longitude with K=5\n",
    "- I will create a new variable called ((City Cluster)) and delet the old one (City)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8a5868f",
   "metadata": {},
   "outputs": [],
   "source": [
    "##City\n",
    "# Select Latitude and Longitude columns for clustering\n",
    "coordinates = data_city\n",
    "\n",
    "# Perform K-means clustering with an arbitrary number of clusters (e.g., 5)\n",
    "kmeans = KMeans(n_clusters=5, random_state=42)\n",
    "data['City Cluster'] = kmeans.fit_predict(coordinates)\n",
    "\n",
    "# Drop city\n",
    "data.drop(['City'], axis=1, inplace=True)\n",
    "\n",
    "\n",
    "# Perform one-hot encoding\n",
    "data = pd.get_dummies(data, columns=catg_to_oneHot, drop_first=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94fac98b",
   "metadata": {},
   "outputs": [],
   "source": [
    "data['City Cluster'].info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef776210",
   "metadata": {},
   "outputs": [],
   "source": [
    "##Online Security & oline Backup & Device Protection & Tech Support\n",
    "data['Online Security'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e33de35",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculation of Confusion Matrix to see if there is any correlation between these variables\n",
    "\n",
    "# Map Online Security & oline Backup & Device Protection & Tech Support to 3 values\n",
    "data['Online Security'] = data['Online Security'].map({'Yes':1, 'No':0, 'No internet service':0})\n",
    "data['Online Backup'] = data['Online Backup'].map({'Yes':1, 'No':0, 'No internet service':0})\n",
    "data['Device Protection'] = data['Device Protection'].map({'Yes':1, 'No':0, 'No internet service':0})\n",
    "data['Tech Support'] = data['Tech Support'].map({'Yes':1, 'No':0, 'No internet service':0})\n",
    "\n",
    "# Compute the correlation matrix\n",
    "correlation_matrix = data[['Online Security', 'Online Backup','Device Protection','Tech Support']].corr()\n",
    "\n",
    "# Display the matrix\n",
    "print(correlation_matrix)\n",
    "\n",
    "# Optional: Plot the heatmap for better visualization\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(5, 4))\n",
    "sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', fmt='.2f')\n",
    "plt.title('Correlation Matrix')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b6e812e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculation of Variance Inflation Factor to see if there is any correlation between these variables (Methos statistic)\n",
    "\n",
    "# Select relevant features\n",
    "X = data[['Online Security', 'Online Backup','Device Protection','Tech Support']]\n",
    "\n",
    "# Add a constant (intercept) to the features\n",
    "X = sm.add_constant(X)\n",
    "\n",
    "# Calculate VIF for each feature\n",
    "vif_data = pd.DataFrame()\n",
    "vif_data[\"Feature\"] = X.columns\n",
    "vif_data[\"VIF\"] = [variance_inflation_factor(X.values, i) for i in range(X.shape[1])]\n",
    "\n",
    "print(vif_data)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a472a718",
   "metadata": {},
   "source": [
    "- Based on Confusion Matrix; we got a Moderate Correlation (~0.5 - 0.8): Some overlap, but both may still add value.\n",
    "- Based on VIF (Variance Inflation Factor); all the variables their VIF between 1 < VIF < 5 and thwy consider as a Moderate correlation so they don't have a strong corelation.\n",
    "- Decision: i will keep all of them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fa85f74",
   "metadata": {},
   "outputs": [],
   "source": [
    "##Streaming Movies & Streaming TV\n",
    "data['Streaming Movies'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07a3b2c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Map Streaming Movies and Streaming TV to 3 values\n",
    "data['Streaming Movies'] = data['Streaming Movies'].map({'Yes': 1, 'No': 0, 'No internet service': 0})\n",
    "data['Streaming TV'] = data['Streaming TV'].map({'Yes': 1, 'No': 0, 'No internet service': 0})\n",
    "\n",
    "# Compute the correlation matrix\n",
    "correlation_matrix = data[['Streaming Movies', 'Streaming TV']].corr()\n",
    "\n",
    "# Display the matrix\n",
    "print(correlation_matrix)\n",
    "\n",
    "# Optional: Plot the heatmap for better visualization\n",
    "\n",
    "plt.figure(figsize=(5, 4))\n",
    "sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', fmt='.2f')\n",
    "plt.title('Correlation Matrix: Streaming Movies & Streaming TV')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a78d142",
   "metadata": {},
   "source": [
    "High Correlation (> 0.8): Indicates multicollinearity. Both features contain redundant information, and you may want to drop one of them.\n",
    "- I will drop (('Streaming Movies'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0d68a98",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.drop('Streaming Movies', axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0a6ece8",
   "metadata": {},
   "source": [
    "\n",
    "# Step 6: Exploratory Data Analysis - Target Variable\n",
    "\n",
    "In this step, we visualize the distribution of the target variable (`Churn Value`) to understand the class balance, which is crucial for model evaluation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b82dab3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualizing the distribution of the target variable 'Churn Value' to understand the class balance.\n",
    "# Understanding class distribution is important for choosing the right evaluation metric and handling class imbalance, if necessary.\n",
    "\n",
    "plt.figure(figsize=(6, 4))\n",
    "sns.countplot(x='Churn Value', data=data, palette='Set2')\n",
    "plt.title('Distribution of Churn Value')\n",
    "plt.xlabel('Churn Value (0 = No Churn, 1 = Churn)')\n",
    "plt.ylabel('Count')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b27c94ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the proportion of Churn vs Non-Churn\n",
    "churn_counts = data['Churn Value'].value_counts()\n",
    "\n",
    "# Plot the pie chart\n",
    "plt.figure(figsize=(6, 6))\n",
    "plt.pie(churn_counts, labels=['No Churn', 'Churn'], autopct='%1.1f%%', startangle=90, colors=['skyblue', 'lightcoral'])\n",
    "plt.title('Churn vs Non-Churn Proportion')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25c2c9e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get a summary of the target variable\n",
    "churn_stats = data['Churn Value'].describe()\n",
    "\n",
    "print(\"Statistics for Churn Value:\")\n",
    "print(churn_stats)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5613116f",
   "metadata": {},
   "source": [
    "- 26.5370 % in the churn rate is means that about 1 in 4 customers in the dataset has churned.\n",
    "- While not perfectly balanced, the churn rate is within a reasonable range (~25-30%), because most standard machine learning algorithms like Logistic Regression, Random Forest ect, can handle this case.\n",
    "- If the churn rate low than 20%, then we have to use Smote or other method to make it balanced.\n",
    "- std = 0.441561 close to 0.5 indicates that the classes are mostly balanced.\n",
    "\n",
    " ---> Introduce Smote methode to balance the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e8a9d27",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Checking for Missing Values:\n",
    "print(\"\\nMissing values per column:\")\n",
    "data.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc3d9313",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Cheking for Outliers:\n",
    "# Create boxplots for each numerical feature\n",
    "for feature in data.columns:\n",
    "    plt.figure(figsize=(8, 4))\n",
    "    sns.boxplot(x=data[feature])\n",
    "    plt.title(f'Boxplot of {feature}')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7dd5eff5",
   "metadata": {},
   "source": [
    "Based on the box plots, we do not have any outliers in the dataset, although some features showed some outliers and after re-examining them, they turned out to be non-outliers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bec5a38",
   "metadata": {},
   "outputs": [],
   "source": [
    "## replacing blank strings with NULL in `Total Charges`\n",
    "data['Total Charges'] = data['Total Charges'].apply(lambda x: np.nan if type(x) == str else x)\n",
    "## confirming NULL values in `Total Charges`\n",
    "data['Total Charges'].isna().sum()\n",
    "\n",
    "data = data.dropna()\n",
    "data['Total Charges'] = data['Total Charges'].astype(float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bcc618f",
   "metadata": {},
   "outputs": [],
   "source": [
    "## fetching duplicated rows (as a whole) in the dataframe\n",
    "data.duplicated().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3485e3b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fetch the duplicated rows\n",
    "duplicated_rows = data[data.duplicated()]\n",
    "\n",
    "# Display duplicated rows\n",
    "print(duplicated_rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "737e382d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove duplicated rows, keeping the first occurrence\n",
    "data = data.drop_duplicates()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30f4c759",
   "metadata": {},
   "source": [
    "\n",
    "# Step 7:  Applying SMOTE and Splitting the Dataset\n",
    "\n",
    "- Resampling the dataset using SMOTE oversampling\n",
    "- We split the data into training and testing sets, typically with an 80/20 split, to evaluate the model's ability to generalize to unseen data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc741caa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We split the data into training and testing sets to evaluate our model's performance on unseen data.\n",
    "# 80% of the data is used for training, while 20% is held back for testing.\n",
    "# This helps in assessing the model's generalization capability.\n",
    "\n",
    "# Define the features (X) and target (y)\n",
    "X = data.drop(['Churn Value'], axis=1)  # Drop target column from features\n",
    "y = data['Churn Value']  # Define the target\n",
    "\n",
    "\n",
    "## creating an instance of SMOTE\n",
    "from imblearn.over_sampling import SMOTE\n",
    "smote = SMOTE(k_neighbors=5, random_state=101, sampling_strategy=1)\n",
    "## resampling the dataset using SMOTE oversampling\n",
    "X, y = smote.fit_resample(X, y)\n",
    "\n",
    "\n",
    "# Split the data (80% Train, 20% Test)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Display the shapes of the datasets\n",
    "print(f\"Train Set: X_train={X_train.shape}, y_train={y_train.shape}\")\n",
    "print(f\"Test Set: X_test={X_test.shape}, y_test={y_test.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f031e81d",
   "metadata": {},
   "source": [
    "\n",
    "# Step 8: Feature Scaling\n",
    "\n",
    "Scaling the features ensures that they are on the same scale, which is particularly important for algorithms like SVM, K-Nearest Neighbors, and others sensitive to feature magnitude.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4451e7d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Many machine learning algorithms, such as Logistic Regression and SVM, benefit from scaled features.\n",
    "# Here we use StandardScaler to standardize features by removing the mean and scaling to unit variance.\n",
    "\n",
    "scaler = StandardScaler()\n",
    "# Fit and transform the training data, and transform the test data\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.fit_transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81d88282",
   "metadata": {},
   "source": [
    "# Step 9: Model Training and Evaluation\n",
    "\n",
    "We will train a Random Forest model to predict customer churn. The performance of the model will be evaluated using metrics such as accuracy, precision, recall, and F1-score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e060516",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_models(seed=42):\n",
    "    '''\n",
    "    Create a list of machine learning models.\n",
    "            Parameters:\n",
    "                    seed (integer): random seed of the models\n",
    "            Returns:\n",
    "                    models (list): list containing the models\n",
    "    '''\n",
    "    models = []\n",
    "    \n",
    "    # Add various models\n",
    "    models.append(('logistic_regression', LogisticRegression(random_state=seed)))\n",
    "    models.append(('support_vector_machines', SVC(random_state=seed)))\n",
    "    models.append(('random_forest', RandomForestClassifier(random_state=seed)))\n",
    "    models.append(('gradient_boosting', GradientBoostingClassifier(random_state=seed)))\n",
    "    models.append(('k_nearest_neighbors', KNeighborsClassifier()))\n",
    "    models.append(('naive_bayes', GaussianNB()))\n",
    "    models.append(('xgboost', XGBClassifier(random_state=seed, use_label_encoder=False, eval_metric='logloss')))\n",
    "    models.append(('lightgbm', LGBMClassifier()))\n",
    "    \n",
    "    return models\n",
    "\n",
    "# Create a list with all the algorithms we are going to assess\n",
    "models = create_models()\n",
    "\n",
    "# Test the accuracy of each model using cross-validation\n",
    "results = []\n",
    "names = []\n",
    "scoring = 'accuracy'\n",
    "\n",
    "for name, model in models:\n",
    "    # Perform cross-validation\n",
    "    score = cross_val_score(model, X_train_scaled, y_train, cv=10, scoring=scoring)\n",
    "    results.append(score)\n",
    "    names.append(name)\n",
    "    # Print classifier accuracy (mean) and standard deviation\n",
    "    print(f'Classifier: {name}, Accuracy: {score.mean():.4f}, Std: {score.std():.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc0c3490",
   "metadata": {},
   "source": [
    "# Step 10: Fine Tuning (Hyperparameter Tuning: RandomSearchCV)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a3c6f68",
   "metadata": {},
   "outputs": [],
   "source": [
    "## defining a function to print classification report and plot the confusion matrix\n",
    "def modelPerformance(model, test, result):\n",
    "    y_pred = model.predict(test)\n",
    "    print(classification_report(result, y_pred))\n",
    "    print('Accuracy - ',accuracy_score(result, y_pred))\n",
    "    print('Recal- ',recall_score(result, y_pred))\n",
    "    print('Precisio - ',precision_score(result, y_pred))\n",
    "    print('F1-Score - ',f1_score(result, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebc8e1fc",
   "metadata": {},
   "source": [
    "#### RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfae4da0",
   "metadata": {},
   "outputs": [],
   "source": [
    "## setting up the hyperparameters for RandomForestClassifier\n",
    "paramsRF = {\n",
    "    \"n_estimators\": np.linspace(64, 256, 10, dtype = int),\n",
    "    \"criterion\": ['gini', 'log_loss'],\n",
    "    \"max_depth\": [6, 8, None],\n",
    "    \"max_features\": ['sqrt', 'log2', None]\n",
    "}\n",
    "## creating the RandomizedSearchCV object\n",
    "randomRF = RandomizedSearchCV(RandomForestClassifier(random_state=101), paramsRF, n_jobs=4, verbose=1, cv=5, scoring='f1', refit=True)\n",
    "## fiting the RandomizedSearchCV model to find the best parameters for Random Forest\n",
    "randomRF.fit(X_train_scaled, y_train)\n",
    "## fetching the best parameters four\n",
    "randomRF.best_params_\n",
    "\n",
    "    \n",
    "## evaluating the result on validation set\n",
    "modelPerformance(randomRF, X_test_scaled, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07bf6479",
   "metadata": {},
   "source": [
    "####  SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e18d31c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "## setting up the hyperparameters for SVC\n",
    "paramsSVC = {\n",
    "    'kernel': ['linear', 'rbf'], \n",
    "    'C': [0.1, 1, 10],\n",
    "    'gamma': ['auto', 'scale']\n",
    "}\n",
    "## creating a RandomizedSearchCV object for SVC\n",
    "randomSVC = RandomizedSearchCV(SVC(random_state=101, max_iter=-1, probability=True), paramsSVC, n_jobs=4, verbose=1, cv=5, scoring = 'f1', refit=True)\n",
    "## fiting the RandomizedSearchCV model to find the best parameters for Random Forest\n",
    "randomSVC.fit(X_train_scaled, y_train)\n",
    "best_randomSVC_model = randomSVC.best_estimator_\n",
    "    \n",
    "## evaluating the result on validation set\n",
    "modelPerformance(best_randomSVC_model, X_test_scaled, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c8d9b37",
   "metadata": {},
   "source": [
    "#### XGBoostClassifier "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d225e066",
   "metadata": {},
   "outputs": [],
   "source": [
    "## setting the hyperparameters for XGBoost \n",
    "paramsXG = {\n",
    "    \"n_estimators\": np.linspace(64, 256, 10, dtype = int),\n",
    "    \"eta\": [0.01, 0.05, 0.1, 0.2],\n",
    "    \"max_depth\": [6, 8, None],\n",
    "    \"subsample\": [0.5, 0.8, 1.0]    \n",
    "}\n",
    "\n",
    "## creating the RandomizedSearchCV object for XGBoostClassifier\n",
    "randomXGB = RandomizedSearchCV(XGBClassifier(seed=101, eval_metric='logloss', objective='binary:logistic'), paramsXG, n_jobs=4, verbose=1, cv=5, scoring='f1', refit=True)\n",
    "## fiting the model\n",
    "randomXGB.fit(X_train_scaled, y_train)\n",
    "best_randomXGB_model = randomXGB.best_estimator_\n",
    "    \n",
    "## evaluating the result on validation set\n",
    "modelPerformance(best_randomXGB_model, X_test_scaled, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a11d1b82",
   "metadata": {},
   "source": [
    "### LGBMClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "833986fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the parameter grid\n",
    "param_grid = {\n",
    "    'num_leaves': [20, 30, 40, 50],\n",
    "    'max_depth': [10, 15, 20, 25],\n",
    "    'learning_rate': [0.01, 0.05, 0.1, 0.15],\n",
    "    'n_estimators': [100, 200, 300, 400],\n",
    "    'min_child_samples': [10, 20, 30],\n",
    "    'subsample': [0.6, 0.8, 1.0],\n",
    "    'colsample_bytree': [0.6, 0.8, 1.0],\n",
    "    'reg_alpha': [0, 0.1, 0.5, 1],\n",
    "    'reg_lambda': [0, 0.1, 0.5, 1]\n",
    "}\n",
    "\n",
    "# Initialize the LGBMClassifier\n",
    "lgbm_model = LGBMClassifier()\n",
    "\n",
    "# Initialize RandomizedSearchCV\n",
    "randomlgbm = RandomizedSearchCV(estimator=lgbm_model, \n",
    "                                   param_distributions=param_grid, \n",
    "                                   n_iter=20, # Number of parameter settings that are sampled\n",
    "                                   cv=5,      # 5-fold cross-validation\n",
    "                                   verbose=1, # To show the progress of the search\n",
    "                                   random_state=42,\n",
    "                                   n_jobs=-1) # Use all processors\n",
    "\n",
    "# Fit the model\n",
    "randomlgbm.fit(X_train_scaled, y_train)\n",
    "best_lgbm_model = randomlgbm.best_estimator_\n",
    "\n",
    "## evaluating the result on validation set\n",
    "modelPerformance(best_lgbm_model, X_test_scaled, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38b4b043",
   "metadata": {},
   "source": [
    "## Model Interpretation \n",
    "\n",
    "In our decision-making process for Churn Prediction models, we choose to go with LGBMClassifie, we chose to use LGBMClassifier, because it provides better metrics than other classifiers.\n",
    "- Accuracy -  0.8506304558680893\n",
    "- Recal-  0.8592092574734812\n",
    "- Precisio -  0.8461538461538461\n",
    "- F1-Score -  0.8526315789473684"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a6407ba",
   "metadata": {},
   "source": [
    "\n",
    "# Step 11: Feature Importance Analysis\n",
    "\n",
    "Analyzing the feature importance helps us understand which features contribute the most to predicting churn, which can provide valuable business insights.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6749a7ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature Importance Analysis\n",
    "# Analyzing feature importances to understand which features contribute most to the prediction of churn.\n",
    "\n",
    "# Get the best estimator (the final trained model)\n",
    "best_model = randomlgbm.best_estimator_\n",
    "\n",
    "# Get feature importances from the best model\n",
    "feature_importances = best_model.feature_importances_\n",
    "\n",
    "# Create a DataFrame for better visualization\n",
    "feature_importance_df = pd.DataFrame({\n",
    "    'Feature': X.columns,\n",
    "    'Importance': feature_importances\n",
    "}).sort_values(by='Importance', ascending=False)\n",
    "\n",
    "# Plot the feature importances\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.barh(feature_importance_df['Feature'], feature_importance_df['Importance'])\n",
    "plt.title('Feature Importances')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a2b050b",
   "metadata": {},
   "source": [
    " Based on the plot, it looks like the most important features are:\n",
    "\n",
    "- Total Charges: This has the highest importance, indicating that it plays a significant role in predicting churn.\n",
    "- Monthly Charges: Also a very important feature.\n",
    "- Tenure Months: The length of time a customer has been with the company is a strong indicator of churn likelihood.\n",
    "- Gender: It also appears to have a moderate impact.\n",
    "- Other features like Internet Service, Payment Method, and Contract have much smaller impacts."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d28fa00",
   "metadata": {},
   "source": [
    "\n",
    "# Step 12: Saving the Model\n",
    "\n",
    "The trained model is saved to disk using joblib. This allows us to reuse the model for predictions without retraining, which is efficient for deployment.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd9fa668",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving the trained model using joblib for future use.\n",
    "# This allows us to easily deploy or re-use the model without retraining it from scratch.\n",
    "\n",
    "# Save the model\n",
    "joblib.dump(best_model, 'lgbm_churn_model.pkl')\n",
    "\n",
    "# To load the model later\n",
    "#loaded_model = joblib.load('lgbm_churn_model.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa44183d",
   "metadata": {},
   "source": [
    "# Step 13: Deploy the Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab6021c2",
   "metadata": {},
   "source": [
    "See app.py file"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
